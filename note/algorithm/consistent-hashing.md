
- 场景
    - 分布式缓存
    - 负载均衡
    - 分布式存储
        - - 分布式集群存储，机器物理故障和业务需要的机器扩缩容导致机器的调整进而带来的数据迁移
- 有读写请求，就需要考虑从哪台机器操作数据，一般有几种方法
    - 随机访问
        - 随机访问可能造成服务器负载压力不均衡；
    - 轮询策略
        - 轮询策略请求均匀分配，但当服务器有性能差异，无法按性能分发
    - 权重轮询策略
        - 权值需要静态配置，无法自动调节；
    - Hash取模策略
        - 哈希取模如果机器动态变化会导致路由产生变化，数据产生大量迁移，导致命中率的急剧下降
        - 规模较小的系统
        - 可以保证相同请求相同机器处理，这是一种并行转串行的方法，如果数据相对独立，就避免了线程间的通信和同步，实现了无锁化处理
        - index = hash_fun(key) % N
            - 如果N不变或者可以自己主动控制，就可以实现数据的负载均衡和无锁化处理，但是一旦N的变化不被控制，那么就会出现问题，数据产生大量迁移
        - 负载均衡场景
            - 对访问者的IP，通过固定算式hash(IP) % N（N为服务器的个数），使得每个IP都可以定位到特定的服务器
    - 一致性哈希策略

- 一致性哈希
    - 核心思想
        - 大量固定数量的小数据块的分片；将普通哈希取模的N进行固定，从而确保了相同的key必然是相同的位置，从而避免了牵一发而动全身的问题
        - 小分片的服务器归属问题；将N固定且设置很大之后，实际上就是进行数据分片Sharding，分布的小片就要和实际的机器产生关联关系，也就是哪台机器负责哪些小分片
        - 解决的问题；不是从彻底解决了由于动态调整服务器数据产生的数据迁移问题，而是将原来普通哈希取模造成的几乎全部迁移，降低为小部分数据的移动
        - 管理员可以根据机器的配置和负载情况进行slot的动态调整，基本上解决了最开始的几种负载均衡策略的不足
    - 少量数据的迁移
        - 一种特殊的哈希算法，这种特殊的哈希算法实现了少量数据的迁移，避免了几乎全部数据的移动，这样就解决了普通hash取模的动态调整带来的全量数据变动
        - 一致性哈希算法并不能杜绝数据迁移的问题，但是可以有效避免数据的全量迁移，需要迁移的只是更改的节点和它的上游节点它们两个节点之间的那部分数据
    - 哈希表槽位数（大小）的改变平均只需要对K/n 个关键字重新映射，其中 K是关键字的数量，n是槽位数量；在传统的哈希表中，添加或删除一个槽位的几乎需要对所有关键字进行重新映射。
    - 哈希取模问题的根源
        - N的变动和相应的数据归属问题
        - 一致性哈希的解决方法
            - 让N很大，每次被移动的key数就是K_all/Slot_n，也就是有槽位的概念，或者说是小分片的概念
            - 小分片的归属
    - 哈希取模方法，它是针对一个点的，业务布局严重依赖于这个计算的点值结果。你结算的结果是2，那么就对应到编号为2的服务器上。这样的映射就造成了业务容错性和可扩展性极低
        - 将这个计算结果的点值赋予范围的意义      
    - 哈希冲突
        -       
            
- Karger的一致性哈希算法
    - 只是提供了一种思想和方向
    - N设置为2^32，形成了一个0~(2^32-1）的哈希环，也就是相当于普通Hash取模时N=2^32；一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环
    - 将数据key进行hash计算时就落在了0~(2^32-1的哈希环上，如果总的key数量为Sum，那么单个哈希环的最小单位上的key数就是 Unit_keys = Sum/2^32
    - 负载均衡场景
        - 服务节点和哈希环分片
            - 将服务器结点也作为一种key分发到哈希环上
                - 假设我们有4台服务器，服务器0、服务器1、服务器2，服务器3，那么，在生产环境中，这4台服务器肯定有自己的 IP 地址或主机名，我们使用它们各自的 IP 地址或主机名作为关键字进行哈希计算，使用哈希后的结果对2^32取模，可以使用公式示意：hash（服务器的IP地址） %  2^32；最后会得到一个 [0, 2^32-1]之间的一个无符号整形数，这个整数就代表服务器的编号。同时这个整数肯定处于[0, 2^32-1]之间，那么，hash 环上必定有一个点与这个整数对应。那么这个服务器就可以映射到这个环上
                - 为用户分配访问的服务器；根据用户的 IP 使用上面相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针行走，遇到的第一台服务器就是其应该定位到的服务器
            - 提高容错性和扩展性
                - 新增服务器节点
                    - 通过上述一致性哈希算法计算后得出它在哈希环的位置
                - 删除服务器节点
                - 分布式节点的变动不会影响整个系统的运行且不需要我们做一些人为的调整策略。
            - 一致性哈希的不均衡(Hash环的数据倾斜问题)
                - 虚拟节点的引入；将服务器结点根据某种规则来虚拟出更多结点，但是这些虚拟节点就相当于服务器的分身
                    - 即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现
                    - 多了一步虚拟节点到实际节点的映射；虚拟节点和实体结点的映射问题
            - 哈希冲突
                - 对于用户请求IP的哈希冲突，其实只是不同用户被分配到了同一台服务器上，这个没什么影响
                - 服务节点哈希冲突会导致两个服务节点在哈希环上对应同一个点
                    - 一方面哈希冲突的概率比较低，另一方面可以通过虚拟节点也可减少这种情况
- Redis的一致性哈希实现
    - Redis并没有使用2^32这种哈希环，而是采用了16384个固定slot来实现的，然后每个服务器Master使用bitmap来确定自己的管辖slot
    - N的取值
        - Redis cluster 拥有固定的16384个slot，slot是虚拟的且被分布到各个master中，当key 映射到某个master 负责slot时，就由对应的master为key 提供服务
    - 分片和服务结点的所属关系 
        - 每个Master节点都维护着一个位序列bitmap为16384/8字节，也就是Master使用bitmap的原理来表征slot的下标，Master 节点通过 bit 来标识哪些槽自己是否拥有，比如对于编号为1的槽，Master只要判断序列的第二位是不是为1即可
- Consistent hashing，其中Consistent译为"一致的，连贯的"，或许连贯的更贴切一些，这种特殊的哈希算法实现了普通哈希取模算法的平滑连贯版本，称为连贯性哈希算法，好像更合适    