

- 单一数据库节点面临的问题
    - 难以支持海量数据存储和高并发查询请求
        - 从性能方面来说，由于关系型数据库大多采用 B+树类型的索引，在数据量逐渐增大的情况下，索引深度的增加也将使得磁盘访问的 IO 次数增加，进而导致查询性能的下降；同时，高并发访问请求也使得集中式数据库成为系统的最大瓶颈
    - 解决方法
        - 在原有关系型数据库的基础上做增量
            - 分库分表方案，使之更好适应海量数据存储和高并发查询请求的场景
                - 产生的问题
                    - 查询时路由库和表，并聚合查询结果
                    - 多表关联查询、排序、分页、事务等等问题
                - Sharding-JDBC，尽量透明化水平分库分表所带来的影响，像查询单数据库实例和单表那样来查询被水平分割的库和表
        - 颠覆关系型数据库
            - 数据存储到原生支持分布式的数据库或NoSQL数据库
            
            
- 在分片的世界里，数据分片有两种法则：垂直拆分和水平拆分。
    - 垂直分片(纵向拆分)
        - 按照业务拆分，专库专用，按照业务将表进行归类，分布到不同的数据库中，从而将压力分散至不同的数据库
        - 无法真正的解决单点瓶颈。如果垂直拆分之后，表中的数据量依然超过单节点所能承载的阈值，则需要水平分片来进一步处理
    - 水平分片又称为横向拆分
        - 通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，每个分片仅包含数据的一部分，使得各个表的数据量保持在阈值以下，是应对高并发和海量数据系统的有效手段
        - 从理论上突破了单机数据量处理的瓶颈，并且扩展相对自由，是分库分表的标准解决方案
        - 水平分库本质上还是在分表，因为被水平拆分后的库中，都有相同的表分片
        - sharing-jdbc
            - 尽量透明化水平分库分表所带来的影响，让使用方尽量像使用一个数据库一样使用水平分片之后的数据库集群，或者像使用一个数据表一样使用水平分片之后的数据表
            - 每个服务器节点只能看到一个逻辑上的数据库节点，和其中的多个逻辑表，它们看不到真正存在于物理世界中的被水平分割的多个数据库分片和被水平分割的多个数据表分片

-  JDBC 层提供的额外服务，可以说是一个增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架
    - 完全兼容 JDBC
    - 用于任何基于 JDBC 的 ORM 框架
    - 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP
    - 支持任意实现 JDBC 规范的数据库
    
- 为透明化水平分库分表所带来的影响所做的处理，让使用方尽量像使用一个数据库一样使用水平分片之后的数据库集群，或者像使用一个数据表一样使用水平分片之后的数据表
    - SQL 解析
        - 解析引擎先通过词法解析器将这句 SQL 拆分为一个个不可再分的单词，再使用语法解析器对 SQL 进行理解，并最终提炼出解析上下文
        - 先使用词法解析器用于将 SQL 拆解为不可再分的原子符号,并将其归类为关键字、表达式、字面量、操作符
        - 再使用语法解析器将 SQL 转换为抽象语法树
        - 解析上下文包括表、选择项、排序项、分组项、聚合函数、分页信息、查询条件以及可能需要修改的占位符的标记
    - SQL 路由
        - 对抽象语法树的遍历去提炼分片所需的上下文，并标记有可能需要改写的位置.
            - 供分片使用的解析上下文包含查询选择项（Select Items）、表信息（Table）、分片条件（Sharding Condition）、自增主键信息（Auto increment Primary Key）、排序信息（Order By）、分组信息（Group By）以及分页信息（Limit、Rownum、Top）
        - 单片路由、多片路由或者全库路由是对路由划分的一种角度，它反映了我最终执行 SQL 的路径有几条
        - 根据库和表配置的分片策略生成路由后的 SQL，路由后的 SQL 有一条或多条，每一条都对应着各自的真实物理分片
    - SQL 改写
        - SQL 改写为在真实数据库中可以正确执行的语句，逻辑 SQL 到物理 SQL 的映射，例如把逻辑表名改成带编号的分片表名
    - SQL 执行
        - 多线程执行器异步执行 SQL 语句
    - 结果归并
        - 多个执行结果集归并以便于通过统一的 JDBC 接口输出
        - 源码 OrderByStreamMergedResult
        - 流式归并
        
-  Groovy 表达式
    - - 对记录的读和写都按照这种方向进行，“方向”，就是分片方式，就是路由
```
ds$->{0..1}.t_order$->{0..1}
ds_${user_id % 2}
t_order_${order_id % 2}
```
    

- 对表两种维度的拆分
    - 数据库维度
        - 有人称这一过程为水平分库，其实它的本质还是在水平地分表，只不过依据表中 user_id 的不同把拆分后的表放入两个数据库实例
    - 表维度
        - t_order.order_id% 2 == 0 的记录全部落到 t_order0，t_order.order_id% 2 == 1 的记录全部落到 t_order1
    - 本质上是分成4张表，分成两组每组分到一个库
    
- 分片策略和分片算法
    - Groovy 表达式
        - 内置的简单算法
    - 分片策略接口和分片算法接口表达更为复杂的分片策略和分片算法
        - 分片算法 怎么根据分片键的值找到对应的分片-物理表
    - 分片算法是分片策略的组成部分，分片策略设置=分片键设置+分片算法设置
    - 分片策略  
        - 两个纬度
            - 分别是数据源分片策略（databaseShardingStrategy）和表分片策略（tableShardingStrategy
                - 数据源分片策略表示数据被路由到目标物理数据库的策略
        - 标准分片策略（使用精确分片算法或者范围分片算法）、复合分片策略（使用符合分片算法）、Hint 分片策略（使用 Hint 分片算法）、Inline 分片策略（使用 Grovvy 表达式作为分片算法）、不分片策略（不使用分片算法）
        - Inline 类型
        -  Inline 类型的行表达式算法
        
- 数据源
    - ShardingSphereDataSource 实现自 JDBC 的标准接口 DataSource
    - ShardingSphereDataSource包装了平常使用的数据源比如Druid，实际上通过ShardingSphereDataSource使用其他第三方的数据库连接池
    - 进而通过这个 ShardingSphereDataSource 调用原生 JDBC 接口来执行 SQL 查询，或者将 dataSource 配置到 JPA，MyBatis 等 ORM 框架来执行 SQL 查询
    - Java 代码调用 ShardingSphereDataSource 实例的 dataSource 的 JDBC 接口时，只感觉自己在对一个逻辑库中的两个逻辑表进行关联查询，并没有意识到物理分片的存在。而背后在进行 SQL 语句的解析、路由、改写、执行和结果归并
    
    
- 逻辑表与物理表
    - 例如，订单表根据主键尾数被水平拆分为 10 张表，分别是 t_order0 到 t_order9，它们的逻辑表名为 t_order，而 t_order0 到 t_order9 就是物理表
- 分片键
    - 例如，若根据订单表中的订单主键的尾数取模结果进行水平分片，则订单主键为分片键。订单表既可以根据单个分片键进行分片，也同样可以根据多个分片键（例如 order_id 和 user_id）进行分片
- 路由
    - 路由 SQL，即根据分片键的值，找到对应的分片表(或分片库)
    - 根据针对逻辑表编写的SQL找到 SQL 语句里包含的查询条件（where ......）所对应的分片（物理表），然后再针对这些分片进行查询，这个找分片的过程叫做路由
- 分片策略
    - 定义如何找到分片-物理表
    - 如何根据分片键的值，找到对应的分片表(或分片库)
    - 分类
        - 标准分片策略    
            - 只支持单分片键
            - 提供对 SQL 语句中的操作符 =、>、 <、>=、<=、IN 和 BETWEEN AND 的分片操支持
            - 两个分片算法
                - PreciseShardingAlgorithm（接口）顾名思义用于处理操作符=和 IN 的精确分片。
                - RangeShardingAlgorithm （接口）顾名思义用于处理操作符 BETWEEN AND、>、<、>=、<= 的范围分片
        - 复合分片策略
            - 支持多分片键，例如对 t_order 表根据 order_id 和 user_id 分片
            - 复合分片策略，和使用了两个维度的标准分片策略不同
        - Hint（翻译为暗示） 分片策略
            - 通过 Hint 指定分片值而非从 SQL 中提取分片值的方式进行分片的策略。SQL 语句中不包含分片值
        - Inline 分片策略/行表达式分片策略
            - 为用 Grovvy 表达式描述的分片算法准备的分片策略
- 分布式主键生成器
    - 内置
        - UUID
        - SNOWFLAKE
            - https://shardingsphere.apache.org/document/legacy/4.x/document/cn/features/sharding/other-features/key-generator/
            - workID 工作进程ID
                - zookeeper
                - mysql
                - ip+port 等等
            - 时钟回拨 
                - 可接受范围内的时间回退-等待；不可接受范围内的时间回退-抛异常；
                - 为每一个正在运行的节点分配一个备用的workID，发生时间回拨时，如果回拨时间较短就休眠等待，回拨时间较长就使用备用的workID。使用备用ID时，再次发送回拨的话就抛出异常
                - 4.X SnowflakeShardingKeyGenerator
        - LEAF
            - Leaf-segment
                - Leaf-segment号段模式是对直接用数据库自增ID充当分布式ID的一种优化，减少对数据库的频率操作。相当于从数据库批量的获取自增ID，每次从数据库取出一个号段范围
                - 两个号段缓存区segment。当前号段已消耗10%时，还没能拿到下一个号段，则会另启一个更新线程去更新下一个号段 
                    - 当号段耗尽时再去DB中取下一个号段，如果此时网络发生抖动，或者DB发生慢查询，业务系统拿不到号段，就会导致整个系统的响应时间变慢，对流量巨大的业务，这是不可容忍的
                    - 所以Leaf在当前号段消费到某个点时，就异步的把下一个号段加载到内存中。而不需要等到号段用尽的时候才去更新号段。这样做很大程度上的降低了系统的风险
                    - 通常推荐号段（segment）长度设置为服务高峰期发号QPS的600倍（10分钟），这样即使DB宕机，Leaf仍能持续发号10-20分钟不受影响
                - 缺点
                    - ID号码不够随机，能够泄露发号数量的信息，不太安全
                    - DB宕机会造成整个系统不可用，以来DB
                - 优点
                    - Leaf服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景；针对不同业务需求，用biz_tag字段来隔离，如果以后需要扩容时，只需对biz_tag分库分表即可
                    - 容灾性高：Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务。 
            - Leaf-snowflake
                - 趋势递增
                - 弱依赖ZooKeeper
            - 目前几乎所有的分布式id生成框架都是有依赖的，因为需要对不同机器上生成的分布式id进行区分，所以每个部署了Leaf服务的ip:port都会有一个workID，Leaf依赖于Zookeeper去给每个ip:port分配一个workID，百度的uid-generator每次机器启动时依赖于数据库的自增键，分配一个workID。只是依赖强弱的区别，Leaf框架目前来看启动时，运行期间都是依赖于Zookeeper的，虽然说运行期间Zookeeper挂了，可以继续运行，但是会导致当前生成id的最大时间戳不能上报
                - 而百度的uid-generator只在启动时依赖数据库，因为每次启动时获得的workID是不同的，下次启动时不需要复用上次的workID。 如果要做到没有依赖，只能是在项目中的property配置文件中写一个映射表，对每个会部署分布式id服务的机器IP:port对应一个workId
    - 分布式主键生成器的接口
        - 自定义的自增主键生成器
    
- 绑定表
    - 指分片规则一致的主表和子表，配置了绑定关系，两张表互为绑定表，绑定表之间的多表关联查询不会出现笛卡尔积关联，关联查询效率将大大提升 