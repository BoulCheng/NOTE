    
##### I/O
- 流
    - I/O是获取、交换信息的渠道，而流是完成I/O操作的主要方式。
    - 流是一种信息的转换，数据和流之间相互转化，可以看作是数据的载体，通过流实现I/O操作，实现数据的传输和交换
    - 流是一种抽象概念，它代表了数据的无结构化传递
   
    - java.io包下流
        - 字节流,InputStream、OutputStream
            - 如果是文件的读写操作，就使用FileInputStream/FileOutputStream；
            - 如果是数组的读写操作，就使用ByteArrayInputStream/ByteArrayOutputStream；
            - 如果是普通字符串的读写操作，就使用BufferedInputStream/BufferedOutputStream
            - 对象，ObjectInputStream/ObjectOutputStream
        - 字符流 Reader、Writer (字符和字节之间需要编解码，为了简易所以才有字符流)
            - 字节流转化字符流 InputStreamReader/OutputStreamWriter
            - 文件 FileReader/FileWriter
            - String StringReader/StringWriter
            - character CharArrayReader / CharArrayWriter
- I/O操作
    - 磁盘I/O操作
        - 内存和磁盘之间的数据传输
    - 网络I/O操作
        - 内存和网络之间的数据传输
    - 传统I/O操作流程
        
        1. 用户空间并通过read系统调用向内核发起读请求；
        2. 内核向硬件(硬件设备包括磁盘、网络)发送读指令，并等待读就绪；
        3. 内核把将要读取的数据复制到指向的内核缓存中；
        4. 操作系统内核将数据复制到用户空间缓冲区，然后read系统调用返回。 
    - JVM通过 FileInputStream#read()可以发起read系统调用, 且InputStream#read()是一个while循环操作，它会一直等待数据读取，直到数据就绪才会返回。 (Reads a byte of data from this input stream. This method blocks if no input is yet available.)
    - 问题
        1. 多次内存复制 数据先从外部设备复制到内核空间，再从内核空间复制到用户空间，这就发生了两次内存复制操作，引起上下文切换 用户态和内核态的相互切换
        2. 阻塞
            - 如果没有数据就绪，这个读取操作将会一直被挂起，用户线程将会处于阻塞状态
            - 大量连接请求时，就需要创建大量监听线程，这时如果线程没有数据就绪就会被挂起，然后进入阻塞状态。一旦发生线程阻塞，这些线程将会不断地抢夺CPU资源，从而导致大量的CPU上下文切换，增加系统的性能开销
    - I/O操作优化
        - 操作系统优化
            - 内核对网络I/O模型的优化-五种I/O模型
        - 语言优化
            - java NIO优化了内存复制以及阻塞导致的严重性能问题，对应零拷贝和I/O复用模型
            - NIO
                - 使用Buffer缓冲区优化读写流操作
                    - NIO, 依然是面向流的(stream-oriented ), 面向Buffer流,基于块以内存块为基本单位处理数据, Buffer是一块连续的内存块用于缓存读写数据; 传统I/O,面向流,基于流的实现以字节为单位处理数据
                    - Buffer可以将文件一次性读入内存再做后续处理，而传统的方式是边读文件边处理数据，BufferedInputStream虽然也有缓存但仍不足
                    - 通道Channel是访问缓冲的接口，用于读取缓冲或者写入数据，表示缓冲数据的源头或者目的地
                - DirectByteBuffer优化多次内存拷贝
                    - 普通的Buffer分配的是JVM堆内存如HeapByteBuffer，而DirectByteBuffer是直接分配物理内存，可以直接访问物理内存
                    - 直接访问物理内存使数据直接在内核空间和外部设备(磁盘、网络)之间传输，减少了用户空间和内核空间之间的内存复制，减少了数据的拷贝
                    
                    - DirectBuffer申请的是非JVM的物理内存，所以创建和销毁的代价很高。DirectBuffer申请的内存并不是直接由JVM负责垃圾回收，但在DirectBuffer包装类被回收时，会通过Java Reference机制来释放该内存块
                - 通道（Channel）和 多路复用器（Selector） 优化阻塞
                    - NIO很多人也称之为Non-block I/O，即非阻塞I/O，因为这样叫，更能体现它的特点
                    - 传统的I/O对Socket的输入流进行读取时，读取流会一直阻塞，直到发生以下三种情况的任意一种才会解除阻塞
                        1. 有数据可读；
                        2. 连接释放；
                        3. 空指针或I/O异常。
                    - 通道（Channel）
                        - 读取和写入数据都要通过Channel，由于Channel是双向的，所以读、写可以同时进行
                        - 传统I/O的数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的I/O接口从磁盘读取或写入; 最开始，在应用程序调用操作系统I/O接口时，是由CPU完成分配，这种方式最大的问题是“发生大量I/O请求时，非常消耗CPU“；之后，操作系统引入了DMA（直接存储器存储），内核空间与磁盘之间的存取完全由DMA负责，但有些情况下会总线冲突
                        - 在实现DMA传输时，是由DMA控制器直接掌管总线，因此，存在着一个总线控制权转移问题。即DMA传输前，CPU要把总线控制权交给DMA控制器，而在结束DMA传输后，DMA控制器应立即把总线控制权再交回给CPU。一个完整的DMA传输过程必须经过DMA请求、DMA响应、DMA传输、DMA结束4个步骤；在DMA连续传输过程中，CPU可以正常工作，CPU一定以某种方式获得了总线的控制权。CPU和DMA时分复用了总线
                        - 通道的出现解决了以上问题，Channel有自己的处理器，可以完成内核空间和磁盘之间的I/O操作，
                    - 多路复用器（Selector）
                        - NIO使用I/O复用器Selector实现非阻塞I/O，Selector就是使用了这五种类型中的I/O复用模型。Java中的Selector其实就是select/poll/epoll的外包类
                        - TCP通信流程中，Socket通信中的connect、accept、read以及write为阻塞操作，在Selector中分别对应SelectionKey的四个监听事件OP_ACCEPT、OP_CONNECT、OP_READ以及OP_WRITE。
                        - 通过Selector来轮询注册在其上的Channel，当发现一个或多个Channel处于就绪状态时，返回就绪的监听事件，最后程序匹配到监听事件，进行相关的I/O操作
                        - 在创建Selector时，程序会根据操作系统版本选择使用哪种I/O复用函数。在JDK1.5版本中，如果程序运行在Linux操作系统，且内核版本在2.6以上，NIO中会选择epoll来替代传统的select/poll，这也极大地提升了NIO通信的性能
                        
                        - Selector是Java NIO编程的基础。用于检查一个或多个NIO Channel的状态是否处于可读、可写。
                        - Selector是基于事件驱动实现的，我们可以在Selector中注册accpet、read监听事件，Selector会不断轮询注册在其上的Channel，如果某个Channel上面发生监听事件，这个Channel就处于就绪状态，然后进行I/O操作。
                        - 一个线程使用一个Selector，通过轮询的方式，可以监听多个Channel上的事件。
                        - 我们可以在注册Channel时设置该通道为非阻塞，当Channel上没有I/O操作时，该线程就不会一直等待了，而是会不断轮询所有Channel，从而避免发生阻塞
                        - 目前操作系统的I/O多路复用机制都使用了epoll，相比传统的select机制，epoll没有最大连接句柄1024的限制。所以Selector在理论上可以轮询成千上万的客户端。
                    - NIO适用于发生大量I/O连接请求的场景
                    - 来源网络待确认 (在Linux中，AIO并未真正使用操作系统所提供的异步I/O，它仍然使用poll或epoll，并将API封装为异步I/O的样子，但是其本质仍然是同步非阻塞I/O，加上第三方产品的出现，Java网络编程明显落后，所以没有成为主流)
        - 用户层线程模型优化
            - NIO是基于事件驱动模型来实现的I/O操作。Reactor模型是同步I/O事件处理的一种常见模型。Reactor线程模型优化。
##### 网络I/O模型(操作系统内核)
- TCP客户端服务端的工作流程(![](../net/tcp/tcp.png) )
    1. 首先，应用程序通过系统调用socket创建一个套接字，它是系统分配给应用程序的一个文件描述符；
    2. 其次，应用程序会通过系统调用bind，绑定地址和端口号，给套接字命名一个名称；
    3. 然后，系统会调用listen创建一个队列用于存放客户端进来的连接；
    4. 最后，应用服务会通过系统调用accept来监听接受客户端的连接请求。
    5. 当有一个客户端连接到服务端之后，服务端就会调用fork创建一个子进程，通过系统调用read监听客户端发来的消息，再通过write向客户端返回信息。

- 五种I/O模型 每一种I/O模型的出现，都是基于前一种I/O模型的优化升级。
1. 阻塞式I/O
    - 在整个socket通信工作流程中，socket的默认状态是阻塞的，当不能立即完成的套接字调用时，其进程将被阻塞，被系统挂起，进入睡眠状态，一直等待相应的操作响应
    - 阻塞的地方
        - connect阻塞，直到确认连接，TCP连接的建立需要完成三次握手过程
        - accept阻塞，服务端接收外来连接，会调用accept函数，如果没有新的连接到达，调用进程将被挂起，进入阻塞状态
        - read、write阻塞，调用read函数等待客户端的数据写入，如果没有数据写入，调用子进程将被挂起，进入阻塞状态
2. 非阻塞式I/O
    - 使用fcntl可以把以上三种操作都设置为非阻塞操作，如果没有数据返回，就会直接返回一个EWOULDBLOCK或EAGAIN错误，此时进程就不会一直被阻塞
    - 需要设置一个线程对该操作进行轮询检查，这也是最传统的非阻塞I/O模型。
    - 用户线程轮询查看一个I/O操作的状态，在大量请求的情况下，会大量消耗CPU
3. I/O复用
    - 在I/O复用模型中，执行读写I/O操作依然是阻塞的，在执行读写I/O操作时，存在着多次内存拷贝和上下文切换，给系统增加了性能开销
    - Linux提供了I/O复用函数select/poll/epoll，进程将一个或多个读操作通过系统调用函数，阻塞在函数操作上，系统内核就可以帮我们侦测多个读操作是否处于就绪状态。
    - select()函数
        - 调用后select() 函数会阻塞，在超时时间内，监听用户感兴趣的文件描述符上的可读可写和异常事件的发生，直到有描述符就绪或者超时，函数返回。返回就绪的文件描述符集合
        - select() 函数监视的文件描述符分3类，分别是writefds（写文件描述符）、readfds（读文件描述符）以及exceptfds（异常事件文件描述符）
        - 通过数组存放文件描述符 且默认1024个 轮训顺序扫描fd是否就绪
        - 包含大量文件描述符的数组被整体复制到用户态和内核的地址空间之间，而无论这些文件描述符是否就绪，他们的开销都会随着文件描述符数量的增加而线性增大
    - poll()函数
        - 没有最大文件描述符数量的限制
    - epoll()函数
        - 红黑树存放文件描述符
        - 没有最大文件描述符数量1024的限制
        - 事件驱动的方式代替轮询扫描fd
4. 信号驱动式I/O
    - 信号驱动式I/O虽然在等待数据就绪时，没有阻塞进程，但在被通知后进行的I/O操作还是阻塞的，进程会等待数据从内核空间复制到用户空间中
    - 信号驱动式I/O相比于前三种I/O模式，实现了在等待数据就绪时，进程不被阻塞，主循环可以继续工作，所以性能更佳。
    - 用户进程不阻塞继续工作，内核数据就绪时，内核就为该进程生成一个SIGIO信号，通过信号回调通知进程进行相关I/O操作。
    - 而由于TCP来说，信号驱动式I/O几乎没有被使用，这是因为SIGIO信号是一种Unix信号，信号没有附加信息，如果一个信号源有多种产生信号的原因，信号接收者就无法确定究竟发生了什么。而 TCP socket生产的信号事件有七种之多，这样应用程序收到 SIGIO，根本无从区分处理
    - 信号驱动式I/O现在被用在了UDP通信
    - UDP只有一个数据请求事件，这也就意味着在正常情况下UDP进程只要捕获SIGIO信号，就调用recvfrom读取到达的数据报。如果出现异常，就返回一个异常错误
5. 异步I/O
    - 等待数据就绪时 和 数据从内核空间复制到用户空间中 都不阻塞
    - 当用户进程发起一个I/O请求操作，系统会告知内核启动某个操作，并让内核在整个操作完成后通知进程。这个操作包括等待数据就绪和数据从内核复制到用户空间
    - 异步I/O的操作系统比较少见（目前Linux暂不支持，而Windows已经实现了异步I/O），所以在实际生产环境中很少用到异步I/O模型。
    
- 由于信号驱动式I/O对TCP通信的不支持，以及异步I/O在Linux操作系统内核中的应用还不大成熟，大部分框架都还是基于I/O复用模型实现的网络通信。

##### 零拷贝
- 零拷贝是一种避免多次内存复制的技术，用来优化读写I/O操作。
- 网络编程中，通常由read、write来完成一次I/O读写操作。每一次I/O读写操作都需要完成四次内存拷贝，路径是I/O设备->内核空间->用户空间->内核空间->其它I/O设备。
- 避免了内核空间与用户空间的数据交换
    - 实现用户空间和内核空间共享一个缓存数据，
    - 用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址，不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理内存地址。
- 具体实现
    1. I/O复用中的epoll函数中就是使用了mmap减少了内存拷贝。Linux内核中的mmap函数可以代替read、write的I/O读写操作，实现用户空间和内核空间共享一个缓存数据
    2. Java的NIO编程中，使用DirectByteBuffer来实现内存的零拷贝。Java直接在JVM内存空间之外开辟了一个物理内存空间，这样内核和用户进程都能共享一份缓存数据 
    
##### 线程模型
- Reactor模型
    - NIO是基于事件驱动模型来实现的I/O操作，Reactor模型是同步I/O事件处理的一种常见模型，其核心思想是将I/O事件注册到多路复用器上，一旦有I/O事件触发，多路复用器就会将事件分发到事件处理器中，执行就绪的I/O事件操作
    - 三个主要组件
        1. 事件接收器Acceptor：主要负责接收请求连接；
        2. 事件分离器Reactor：接收请求后，会将建立的连接注册到分离器中，依赖于循环监听多路复用器Selector，一旦监听到事件，就会将事件dispatch到事件处理器；
        3. 事件处理器Handlers：事件处理器主要是完成相关的事件处理，比如读写I/O操作。
        
- 三种模型
    1. 单线程Reactor线程模型
        - 所有的I/O操作都是在一个NIO线程上完成
        - 但NIO读写I/O操作时用户进程还是处于阻塞状态，这种方式在高负载、高并发的场景下会存在性能瓶颈，一个NIO线程如果同时处理上万连接的I/O操作，系统是无法支撑这种量级的请求的
    2. 多线程Reactor线程模型
        - Netty中都使用了一个Acceptor线程来监听连接请求事件
        - 当连接成功之后，会将建立的连接注册到多路复用器中，一旦监听到事件，将交给Worker线程池来负责处理
        - 大多数情况下，这种线程模型可以满足性能要求，但如果连接的客户端再上一个量级，一个Acceptor线程可能会存在性能瓶颈
    3. 主从Reactor线程模型
        - 主流通信框架中的NIO通信框架都是基于主从Reactor线程模型来实现的
        - Acceptor不再是一个单独的NIO线程，而是一个线程池
        - Acceptor接收到客户端的TCP连接请求，建立连接之后，后续的I/O操作将交给Worker I/O线程。
- 实际应用
    - netty
        - Netty中的三种线程模型对应Reactor模型三种模型
        - boss thread只有一个, 但设计成thread pool的原因
            - netty服务端开启多个端口，需要多个boss线程，此时可以使用thread pool
            - 通过线程池管理boss线程
        - 开启一个端口，即使是主从Reactor模型，其实在Netty中，bossGroup线程池最终还是只会随机选择一个线程用于处理客户端连接，与此同时，NioServerSocetChannel绑定到bossGroup的线程中，NioSocketChannel绑定到workGroup的线程中
    - Tomcat
    